## Bayesian Optimization - https://arxiv.org/pdf/1807.02811.pdf

covariance

## Maximum Likelihood Estimate
one use case
>[!INFO]- how geometric error is MLE?
>![[geometric_error_as_MLE.png]]

## Least Squares


Jensen's Inequality

Lipschitz Constant

Line search method

condition

Pre conditioner using Hessian

## Newton's Method 
	includes Hessian for pre conditioning

![[newton_method_hessian.png]]


[[Newton Raphson Method]]
o

## Gauss Newton Method 

[Deep Learning Optimizers. SGD with momentum, Adagrad, Adadeltaâ€¦ | by Gunand Mayanglambam | Towards Data Science](https://towardsdatascience.com/deep-learning-optimizers-436171c9e23f)

- doeesnt need second derivatives which are needed in Newton method
![[gauss_newton.png]]


## Gradient Descent

![[grad_descent.png]]

Stochastic Gradient descent
Minibatch stochastic gradient descent
AdaGrad
Adadelta
Adam

## Levenberg-Marquardt (LM) ^de3843

LM or DLS (Damped Least Squares)
somewhere in between Gauss Newton Method and Gradient Descent
- [ ] ![[LM.png]]lev