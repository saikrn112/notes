Bayesian Optimization - https://arxiv.org/pdf/1807.02811.pdf

covariance

Maximum Likelihood Estimate


Least Squares


Jensen's Inequality

Lipschitz Constant

Line search method

condition

Pre conditioner using Hessian

Newton's Method 
	includes Hessian for pre conditioning

[[Newton Raphson Method]]
o

Gauss Neton Method 

[Deep Learning Optimizers. SGD with momentum, Adagrad, Adadeltaâ€¦ | by Gunand Mayanglambam | Towards Data Science](https://towardsdatascience.com/deep-learning-optimizers-436171c9e23f)

Gradient Descent
Stochastic Gradient descent
Minibatch stochastic gradient descent
AdaGrad
Adadelta
Adam

Levenberg-Marquardt
